> python main.py --data_dir ./data --epochs 10 --batch_size 16
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ğŸš€ 3D Bounding Box Prediction â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Starting Data Preprocessing...
âŒ Error loading sample from ./data/9bccdb81-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/96e66c6b-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/8fc581d1-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/a0818ba8-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/936fee38-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/889a9fb2-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/936fee3e-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/8eaf3ee7-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/889a9fb9-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/983022aa-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/a0818ba5-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
Data preprocessing complete! Loaded 189 samples. 
Starting Model Training...
ğŸ”„ Training Epoch 1  â”ƒ Batch: 1 ended with 0.7124 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 2 ended with 0.6669 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 3 ended with 0.6322 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 4 ended with 0.6642 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 5 ended with 0.5997 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 6 ended with 0.6906 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 7 ended with 0.5810 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 8 ended with 0.4661 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 9 ended with 0.5793 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 10 ended with 0.5433 loss.
Epoch: 1 ended with loss 0.6135652661323547. 
ğŸ”„ Training Epoch 2  â”ƒ Batch: 1 ended with 0.6704 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 2 ended with 0.5344 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 3 ended with 0.5969 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 4 ended with 0.5093 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 5 ended with 0.6451 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 6 ended with 0.4600 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 7 ended with 0.5837 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 8 ended with 0.4645 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 9 ended with 0.4699 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 10 ended with 0.4932 loss.
Epoch: 2 ended with loss 0.5427473187446594. 
ğŸ”„ Training Epoch 3  â”ƒ Batch: 1 ended with 0.5093 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 2 ended with 0.4420 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 3 ended with 0.4450 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 4 ended with 0.4823 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 5 ended with 0.4467 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 6 ended with 0.4365 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 7 ended with 0.4204 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 8 ended with 0.4875 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 9 ended with 0.4870 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 10 ended with 0.4107 loss.
Epoch: 3 ended with loss 0.45674389600753784. 
ğŸ”„ Training Epoch 4  â”ƒ Batch: 1 ended with 0.4824 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 2 ended with 0.3759 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 3 ended with 0.3530 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 4 ended with 0.3207 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 5 ended with 0.3694 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 6 ended with 0.3230 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 7 ended with 0.3212 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 8 ended with 0.3309 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 9 ended with 0.2438 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 10 ended with 0.2652 loss.
Epoch: 4 ended with loss 0.3385637700557709. 
ğŸ”„ Training Epoch 5  â”ƒ Batch: 1 ended with 0.2366 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 2 ended with 0.2399 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 3 ended with 0.2503 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 4 ended with 0.2386 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 5 ended with 0.3054 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 6 ended with 0.2648 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 7 ended with 0.2737 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 8 ended with 0.2541 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 9 ended with 0.2476 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 10 ended with 0.2785 loss.
Epoch: 5 ended with loss 0.25894418358802795. 
ğŸ”„ Training Epoch 6  â”ƒ Batch: 1 ended with 0.1988 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 2 ended with 0.2659 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 3 ended with 0.2187 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 4 ended with 0.2392 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 5 ended with 0.2149 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 6 ended with 0.2455 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 7 ended with 0.2375 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 8 ended with 0.2151 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 9 ended with 0.1712 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 10 ended with 0.1717 loss.
Epoch: 6 ended with loss 0.21785052120685577. 
ğŸ”„ Training Epoch 7  â”ƒ Batch: 1 ended with 0.1908 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 2 ended with 0.1717 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 3 ended with 0.1812 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 4 ended with 0.2077 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 5 ended with 0.1881 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 6 ended with 0.2053 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 7 ended with 0.1962 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 8 ended with 0.1887 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 9 ended with 0.1787 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 10 ended with 0.2028 loss.
Epoch: 7 ended with loss 0.19112303853034973. 
ğŸ”„ Training Epoch 8  â”ƒ Batch: 1 ended with 0.1479 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 2 ended with 0.2201 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 3 ended with 0.2220 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 4 ended with 0.1556 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 5 ended with 0.1720 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 6 ended with 0.1827 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 7 ended with 0.1695 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 8 ended with 0.1532 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 9 ended with 0.1664 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 10 ended with 0.1576 loss.
Epoch: 8 ended with loss 0.17468824982643127. 
ğŸ”„ Training Epoch 9  â”ƒ Batch: 1 ended with 0.1810 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 2 ended with 0.1546 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 3 ended with 0.1664 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 4 ended with 0.1487 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 5 ended with 0.1607 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 6 ended with 0.1848 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 7 ended with 0.1762 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 8 ended with 0.1649 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 9 ended with 0.1542 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 10 ended with 0.1602 loss.
Epoch: 9 ended with loss 0.16517136991024017. 
ğŸ”„ Training Epoch 10  â”ƒ Batch: 1 ended with 0.1643 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 2 ended with 0.1526 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 3 ended with 0.1574 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 4 ended with 0.1422 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 5 ended with 0.1610 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 6 ended with 0.1682 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 7 ended with 0.1679 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 8 ended with 0.1595 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 9 ended with 0.1538 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 10 ended with 0.1520 loss.
Epoch: 10 ended with loss 0.15788349509239197. 
Model Training complete! 
ğŸ”„ Starting Evaluation ... 
Model Evaluation Complete. Loss obtained: 0.1438378095626831. 
Model Saved Successfully!