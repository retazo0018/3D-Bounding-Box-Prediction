> python main.py --data_dir ./data --epochs 15 --batch_size 16
/Users/ashwin/miniconda3/envs/nerf/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.1'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ğŸš€ 3D Bounding Box Prediction â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Starting Data Preprocessing...
/Users/ashwin/Professional/coding-challenges/Sereact/perception/data_prepare.py:41: UserWarning: Argument 'var_limit' is not valid and will be ignored.
  A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),
âŒ Error loading sample from ./data/9bccdb81-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/96e66c6b-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/8fc581d1-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/a0818ba8-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/936fee38-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/889a9fb2-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/936fee3e-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/8eaf3ee7-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/889a9fb9-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/983022aa-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
âŒ Error loading sample from ./data/a0818ba5-9915-11ee-9103-bbb8eae05561: Trying to create tensor with negative dimension -615: [-615, 640, 640]
Data preprocessing complete! Loaded 378 samples. 
Model Summary: 
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
MultiObject3DBBoxModel                                  [1, 25, 8, 3]             2,049,000
â”œâ”€Sequential: 1-1                                       [1, 2048, 16, 16]         --
â”‚    â””â”€Conv2d: 2-1                                      [1, 64, 256, 256]         (9,408)
â”‚    â””â”€BatchNorm2d: 2-2                                 [1, 64, 256, 256]         (128)
â”‚    â””â”€ReLU: 2-3                                        [1, 64, 256, 256]         --
â”‚    â””â”€MaxPool2d: 2-4                                   [1, 64, 128, 128]         --
â”‚    â””â”€Sequential: 2-5                                  [1, 256, 128, 128]        --
â”‚    â”‚    â””â”€Bottleneck: 3-1                             [1, 256, 128, 128]        (75,008)
â”‚    â”‚    â””â”€Bottleneck: 3-2                             [1, 256, 128, 128]        (70,400)
â”‚    â”‚    â””â”€Bottleneck: 3-3                             [1, 256, 128, 128]        (70,400)
â”‚    â””â”€Sequential: 2-6                                  [1, 512, 64, 64]          --
â”‚    â”‚    â””â”€Bottleneck: 3-4                             [1, 512, 64, 64]          (379,392)
â”‚    â”‚    â””â”€Bottleneck: 3-5                             [1, 512, 64, 64]          (280,064)
â”‚    â”‚    â””â”€Bottleneck: 3-6                             [1, 512, 64, 64]          (280,064)
â”‚    â”‚    â””â”€Bottleneck: 3-7                             [1, 512, 64, 64]          (280,064)
â”‚    â””â”€Sequential: 2-7                                  [1, 1024, 32, 32]         --
â”‚    â”‚    â””â”€Bottleneck: 3-8                             [1, 1024, 32, 32]         (1,512,448)
â”‚    â”‚    â””â”€Bottleneck: 3-9                             [1, 1024, 32, 32]         (1,117,184)
â”‚    â”‚    â””â”€Bottleneck: 3-10                            [1, 1024, 32, 32]         (1,117,184)
â”‚    â”‚    â””â”€Bottleneck: 3-11                            [1, 1024, 32, 32]         (1,117,184)
â”‚    â”‚    â””â”€Bottleneck: 3-12                            [1, 1024, 32, 32]         (1,117,184)
â”‚    â”‚    â””â”€Bottleneck: 3-13                            [1, 1024, 32, 32]         (1,117,184)
â”‚    â””â”€Sequential: 2-8                                  [1, 2048, 16, 16]         --
â”‚    â”‚    â””â”€Bottleneck: 3-14                            [1, 2048, 16, 16]         (6,039,552)
â”‚    â”‚    â””â”€Bottleneck: 3-15                            [1, 2048, 16, 16]         (4,462,592)
â”‚    â”‚    â””â”€Bottleneck: 3-16                            [1, 2048, 16, 16]         (4,462,592)
â”œâ”€PCFeatureExtractor: 1-2                               [1, 128, 16, 16]          --
â”‚    â””â”€Sequential: 2-9                                  [1, 128, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-17                                [1, 32, 16, 16]           896
â”‚    â”‚    â””â”€BatchNorm2d: 3-18                           [1, 32, 16, 16]           64
â”‚    â”‚    â””â”€ReLU: 3-19                                  [1, 32, 16, 16]           --
â”‚    â”‚    â””â”€Conv2d: 3-20                                [1, 64, 16, 16]           18,496
â”‚    â”‚    â””â”€BatchNorm2d: 3-21                           [1, 64, 16, 16]           128
â”‚    â”‚    â””â”€ReLU: 3-22                                  [1, 64, 16, 16]           --
â”‚    â”‚    â””â”€Conv2d: 3-23                                [1, 128, 16, 16]          73,856
â”‚    â”‚    â””â”€BatchNorm2d: 3-24                           [1, 128, 16, 16]          256
â”‚    â”‚    â””â”€ReLU: 3-25                                  [1, 128, 16, 16]          --
â”œâ”€TransformerFeatureFusion: 1-3                         --                        --
â”‚    â””â”€Linear: 2-10                                     [1, 512]                  1,049,088
â”‚    â””â”€Linear: 2-11                                     [1, 512]                  66,048
â”‚    â””â”€TransformerEncoder: 2-12                         [1, 2, 512]               --
â”‚    â”‚    â””â”€ModuleList: 3-26                            --                        6,304,768
â”‚    â””â”€Linear: 2-13                                     [1, 512]                  262,656
â”œâ”€CenterPredictor: 1-4                                  --                        --
â”‚    â””â”€Sequential: 2-14                                 [1, 75]                   --
â”‚    â”‚    â””â”€Linear: 3-27                                [1, 256]                  131,328
â”‚    â”‚    â””â”€ReLU: 3-28                                  [1, 256]                  --
â”‚    â”‚    â””â”€Linear: 3-29                                [1, 75]                   19,275
â”œâ”€BBoxRegressor: 1-5                                    --                        --
â”‚    â””â”€Sequential: 2-15                                 [1, 600]                  --
â”‚    â”‚    â””â”€Linear: 3-30                                [1, 256]                  19,456
â”‚    â”‚    â””â”€ReLU: 3-31                                  [1, 256]                  --
â”‚    â”‚    â””â”€Linear: 3-32                                [1, 600]                  154,200
=========================================================================================================
Total params: 33,657,547
Trainable params: 10,149,515
Non-trainable params: 23,508,032
Total mult-adds (Units.GIGABYTES): 21.38
=========================================================================================================
Input size (MB): 32.51
Forward/backward pass size (MB): 930.09
Params size (MB): 118.03
Estimated Total Size (MB): 1080.63
=========================================================================================================
Starting Model Training...
ğŸ”„ Training Epoch 1  â”ƒ Batch: 1 ended with 0.6935 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 2 ended with 0.5617 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 3 ended with 0.5230 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 4 ended with 0.6005 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 5 ended with 0.6387 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 6 ended with 0.6774 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 7 ended with 0.5903 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 8 ended with 0.5106 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 9 ended with 0.5795 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 10 ended with 0.6506 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 11 ended with 0.6630 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 12 ended with 0.5651 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 13 ended with 0.6746 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 14 ended with 0.5958 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 15 ended with 0.5982 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 16 ended with 0.5682 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 17 ended with 0.5020 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 18 ended with 0.5790 loss.
ğŸ”„ Training Epoch 1  â”ƒ Batch: 19 ended with 0.5916 loss.
Epoch: 1 ended with loss 0.5980644226074219. 
ğŸ”„ Training Epoch 2  â”ƒ Batch: 1 ended with 0.6166 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 2 ended with 0.5760 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 3 ended with 0.6302 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 4 ended with 0.5923 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 5 ended with 0.5834 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 6 ended with 0.6151 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 7 ended with 0.5805 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 8 ended with 0.4928 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 9 ended with 0.5506 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 10 ended with 0.5027 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 11 ended with 0.5836 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 12 ended with 0.4810 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 13 ended with 0.5700 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 14 ended with 0.5860 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 15 ended with 0.7238 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 16 ended with 0.4613 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 17 ended with 0.5331 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 18 ended with 0.5869 loss.
ğŸ”„ Training Epoch 2  â”ƒ Batch: 19 ended with 0.4509 loss.
Epoch: 2 ended with loss 0.5640463829040527. 
ğŸ”„ Training Epoch 3  â”ƒ Batch: 1 ended with 0.5132 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 2 ended with 0.4608 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 3 ended with 0.5551 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 4 ended with 0.5084 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 5 ended with 0.4847 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 6 ended with 0.4777 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 7 ended with 0.4677 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 8 ended with 0.5129 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 9 ended with 0.4681 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 10 ended with 0.5029 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 11 ended with 0.4901 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 12 ended with 0.5561 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 13 ended with 0.5375 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 14 ended with 0.4253 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 15 ended with 0.5038 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 16 ended with 0.4524 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 17 ended with 0.4501 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 18 ended with 0.4073 loss.
ğŸ”„ Training Epoch 3  â”ƒ Batch: 19 ended with 0.3861 loss.
Epoch: 3 ended with loss 0.48211154341697693. 
ğŸ”„ Training Epoch 4  â”ƒ Batch: 1 ended with 0.5395 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 2 ended with 0.4353 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 3 ended with 0.4030 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 4 ended with 0.3854 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 5 ended with 0.3535 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 6 ended with 0.3900 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 7 ended with 0.3821 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 8 ended with 0.4090 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 9 ended with 0.3770 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 10 ended with 0.3130 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 11 ended with 0.3811 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 12 ended with 0.3376 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 13 ended with 0.2826 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 14 ended with 0.3708 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 15 ended with 0.3195 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 16 ended with 0.3670 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 17 ended with 0.3809 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 18 ended with 0.3653 loss.
ğŸ”„ Training Epoch 4  â”ƒ Batch: 19 ended with 0.3816 loss.
Epoch: 4 ended with loss 0.37758868932724. 
ğŸ”„ Training Epoch 5  â”ƒ Batch: 1 ended with 0.3157 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 2 ended with 0.3667 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 3 ended with 0.3227 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 4 ended with 0.3393 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 5 ended with 0.3397 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 6 ended with 0.3777 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 7 ended with 0.3311 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 8 ended with 0.3380 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 9 ended with 0.3258 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 10 ended with 0.3024 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 11 ended with 0.3183 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 12 ended with 0.3336 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 13 ended with 0.2750 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 14 ended with 0.3231 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 15 ended with 0.3017 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 16 ended with 0.2943 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 17 ended with 0.2923 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 18 ended with 0.3054 loss.
ğŸ”„ Training Epoch 5  â”ƒ Batch: 19 ended with 0.2621 loss.
Epoch: 5 ended with loss 0.31921300292015076. 
ğŸ”„ Training Epoch 6  â”ƒ Batch: 1 ended with 0.2997 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 2 ended with 0.2767 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 3 ended with 0.3135 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 4 ended with 0.2678 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 5 ended with 0.2723 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 6 ended with 0.2607 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 7 ended with 0.2729 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 8 ended with 0.2672 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 9 ended with 0.2691 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 10 ended with 0.2621 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 11 ended with 0.2530 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 12 ended with 0.2653 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 13 ended with 0.2591 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 14 ended with 0.2597 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 15 ended with 0.2468 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 16 ended with 0.2977 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 17 ended with 0.2512 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 18 ended with 0.2688 loss.
ğŸ”„ Training Epoch 6  â”ƒ Batch: 19 ended with 0.2643 loss.
Epoch: 6 ended with loss 0.2698948085308075. 
ğŸ”„ Training Epoch 7  â”ƒ Batch: 1 ended with 0.2907 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 2 ended with 0.2502 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 3 ended with 0.2606 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 4 ended with 0.2412 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 5 ended with 0.2800 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 6 ended with 0.2196 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 7 ended with 0.2666 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 8 ended with 0.2168 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 9 ended with 0.2391 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 10 ended with 0.2347 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 11 ended with 0.2332 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 12 ended with 0.2402 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 13 ended with 0.2327 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 14 ended with 0.2181 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 15 ended with 0.1994 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 16 ended with 0.2220 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 17 ended with 0.2328 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 18 ended with 0.2935 loss.
ğŸ”„ Training Epoch 7  â”ƒ Batch: 19 ended with 0.2270 loss.
Epoch: 7 ended with loss 0.2420283555984497. 
ğŸ”„ Training Epoch 8  â”ƒ Batch: 1 ended with 0.2298 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 2 ended with 0.2413 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 3 ended with 0.2335 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 4 ended with 0.2397 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 5 ended with 0.2631 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 6 ended with 0.2374 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 7 ended with 0.2318 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 8 ended with 0.2196 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 9 ended with 0.2306 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 10 ended with 0.2674 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 11 ended with 0.2234 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 12 ended with 0.2250 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 13 ended with 0.2011 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 14 ended with 0.2115 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 15 ended with 0.2228 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 16 ended with 0.2298 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 17 ended with 0.2229 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 18 ended with 0.2249 loss.
ğŸ”„ Training Epoch 8  â”ƒ Batch: 19 ended with 0.2351 loss.
Epoch: 8 ended with loss 0.23110052943229675. 
ğŸ”„ Training Epoch 9  â”ƒ Batch: 1 ended with 0.2015 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 2 ended with 0.2453 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 3 ended with 0.2020 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 4 ended with 0.1928 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 5 ended with 0.2192 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 6 ended with 0.2071 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 7 ended with 0.2166 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 8 ended with 0.2332 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 9 ended with 0.2171 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 10 ended with 0.2116 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 11 ended with 0.1977 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 12 ended with 0.2428 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 13 ended with 0.2325 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 14 ended with 0.2056 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 15 ended with 0.2261 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 16 ended with 0.2375 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 17 ended with 0.2307 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 18 ended with 0.2573 loss.
ğŸ”„ Training Epoch 9  â”ƒ Batch: 19 ended with 0.2234 loss.
Epoch: 9 ended with loss 0.22104695439338684. 
ğŸ”„ Training Epoch 10  â”ƒ Batch: 1 ended with 0.2381 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 2 ended with 0.2399 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 3 ended with 0.2317 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 4 ended with 0.2619 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 5 ended with 0.2119 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 6 ended with 0.2092 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 7 ended with 0.2002 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 8 ended with 0.2274 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 9 ended with 0.2115 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 10 ended with 0.1973 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 11 ended with 0.2152 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 12 ended with 0.2348 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 13 ended with 0.1891 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 14 ended with 0.2013 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 15 ended with 0.2088 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 16 ended with 0.2386 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 17 ended with 0.2132 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 18 ended with 0.2532 loss.
ğŸ”„ Training Epoch 10  â”ƒ Batch: 19 ended with 0.1887 loss.
Epoch: 10 ended with loss 0.2195798009634018. 
ğŸ”„ Training Epoch 11  â”ƒ Batch: 1 ended with 0.1949 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 2 ended with 0.2218 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 3 ended with 0.2191 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 4 ended with 0.2059 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 5 ended with 0.2145 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 6 ended with 0.2179 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 7 ended with 0.2480 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 8 ended with 0.2305 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 9 ended with 0.2505 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 10 ended with 0.1913 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 11 ended with 0.2278 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 12 ended with 0.2318 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 13 ended with 0.2223 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 14 ended with 0.2193 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 15 ended with 0.2023 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 16 ended with 0.2112 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 17 ended with 0.1964 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 18 ended with 0.2030 loss.
ğŸ”„ Training Epoch 11  â”ƒ Batch: 19 ended with 0.1935 loss.
Epoch: 11 ended with loss 0.21589186787605286. 
ğŸ”„ Training Epoch 12  â”ƒ Batch: 1 ended with 0.1911 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 2 ended with 0.2277 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 3 ended with 0.1989 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 4 ended with 0.1991 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 5 ended with 0.2042 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 6 ended with 0.2095 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 7 ended with 0.2134 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 8 ended with 0.1946 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 9 ended with 0.1900 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 10 ended with 0.2436 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 11 ended with 0.2390 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 12 ended with 0.1956 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 13 ended with 0.2037 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 14 ended with 0.2417 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 15 ended with 0.2440 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 16 ended with 0.2045 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 17 ended with 0.2524 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 18 ended with 0.2226 loss.
ğŸ”„ Training Epoch 12  â”ƒ Batch: 19 ended with 0.2058 loss.
Epoch: 12 ended with loss 0.21481458842754364. 
ğŸ”„ Training Epoch 13  â”ƒ Batch: 1 ended with 0.2155 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 2 ended with 0.2056 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 3 ended with 0.1926 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 4 ended with 0.2344 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 5 ended with 0.2086 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 6 ended with 0.2451 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 7 ended with 0.2125 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 8 ended with 0.2079 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 9 ended with 0.2321 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 10 ended with 0.2153 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 11 ended with 0.1922 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 12 ended with 0.1957 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 13 ended with 0.2119 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 14 ended with 0.1913 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 15 ended with 0.2220 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 16 ended with 0.2221 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 17 ended with 0.2212 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 18 ended with 0.1929 loss.
ğŸ”„ Training Epoch 13  â”ƒ Batch: 19 ended with 0.2465 loss.
Epoch: 13 ended with loss 0.21396130323410034. 
ğŸ”„ Training Epoch 14  â”ƒ Batch: 1 ended with 0.1930 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 2 ended with 0.2228 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 3 ended with 0.1885 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 4 ended with 0.2324 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 5 ended with 0.2050 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 6 ended with 0.2024 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 7 ended with 0.2074 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 8 ended with 0.2340 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 9 ended with 0.2472 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 10 ended with 0.1901 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 11 ended with 0.2194 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 12 ended with 0.2188 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 13 ended with 0.2197 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 14 ended with 0.1962 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 15 ended with 0.2119 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 16 ended with 0.2175 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 17 ended with 0.2041 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 18 ended with 0.2066 loss.
ğŸ”„ Training Epoch 14  â”ƒ Batch: 19 ended with 0.1957 loss.
Epoch: 14 ended with loss 0.21119120717048645. 
ğŸ”„ Training Epoch 15  â”ƒ Batch: 1 ended with 0.2024 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 2 ended with 0.2048 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 3 ended with 0.1987 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 4 ended with 0.1896 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 5 ended with 0.2129 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 6 ended with 0.2277 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 7 ended with 0.2110 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 8 ended with 0.2074 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 9 ended with 0.2076 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 10 ended with 0.2276 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 11 ended with 0.2196 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 12 ended with 0.1802 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 13 ended with 0.2157 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 14 ended with 0.1937 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 15 ended with 0.2136 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 16 ended with 0.2070 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 17 ended with 0.2193 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 18 ended with 0.2085 loss.
ğŸ”„ Training Epoch 15  â”ƒ Batch: 19 ended with 0.1909 loss.
Epoch: 15 ended with loss 0.2072741836309433. 
Model Training complete! 
ğŸ”„ Starting Evaluation ... 
Model Evaluation Complete. Loss obtained: 0.22011268138885498. 
Model Saved Successfully!